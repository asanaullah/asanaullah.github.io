I"<p>Field Programmable Gate Arrays (FPGAs) are rapidly becoming first-class citizens in the datacenter, instead of niche components. This is because FPGAs i) can achieve high throughput and low latency by implementing a specialized architecture that eliminates a number of bottlenecks and overheads of general purpose computing, ii) consume little power and, by extension, have a high power-performance ratio, iii) have high-speed interconnects and can tightly couple computation with communication to mask the latency of data movement, and iv) can be configured so that each design is tuned for individual use cases.</p>

<p>Figure 1 illustrates the different configurations in which FPGAs are being deployed in a datacenter. Bump-in-the-Wire (BitW) FPGAs process all traffic between a server and a switch to perform application and system function acceleration. Coprocessor FPGAs provide a traditional accelerator configuration, like GPUs, with an optional back-end secondary network for direct connectivity between accelerators. Storage-attached FPGAs process data locally on storage servers to avoid memory copies to compute servers. Stand-alone FPGAs provide a pool of reconfigurable accelerators that can be programmed and interfaced with directly over the network. Smart network interface controllers (NICs) contain embedded FPGAs which perform custom packet processing alongside a NIC ASIC (application-specific integrated circuit). Finally, network switches can also contain embedded FPGAs that process data as the data moves through the datacenter network (e.g., collective operations such as broadcast and all-reduce).</p>

<div style="text-align:center;">
<img src="/images/blog/overview/fig1.png" style="width:60%;" />
<p> Figure 1: Different configurations for deploying FPGAs in Data Centers </p>
</div>

<h7>Research problem</h7>
<p>FPGAs have traditionally lacked the clean, coherent, compatible, and consistent support for code generation and deployment generation that is typically available for traditional central processing units (CPUs). For the most part, previous efforts to address this have been ad hoc and limited in scope. Those who try to address this always do something special due to poor tooling, and the tooling that does exist is insufficient, especially for datacenter and high-performance computing (HPC) applications. This is because of the heavy reliance on proprietary, vendor-specific tools for core operations. These tools can change frequently and significantly, which means that even if we wanted to not be ad hoc, for the most part we couldn‚Äôt be without investing a lot of work which could be wiped out with a new generation of FPGAs. Moreover, these tools are not necessarily aimed at providing the most efficient solution since they limit the ‚Äúflexibility‚Äù offered to users. For example, these tools i) do not allow modifications the algorithms for core operations (such as logic optimization and place &amp; route), ii) hide details of (and access to) the underlying device architecture which prevents implementation of important functions (such as logic relocation without recompilation), and iii) are designed to be generic with limited opportunities for customization (such vendor IP blocks). This is not a problem unique to FPGAs, however. Similar issues already exist for software. Therefore, similar to free software in the software world, we must be able to code and deploy custom architectures using transparent, open, end-to-end frameworks that are i) not tied to any vendor, that is, do not use IP blocks or tools that are only compatible with the FPGA boards of a particular vendor, ii) provide opportunities for customization across all levels of the development stack, and iii) can be upstreamed, that is, can be easily and reliably integrated into downstream projects in order to build more complex and intricate systems.</p>

<h7>Hardware as a reconfigurable, elastic, and specialized service</h7>
:ET